{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Understanding The Metric: Quadratic Weighted Kappa (QWK)","metadata":{}},{"cell_type":"markdown","source":"![](https://storage.googleapis.com/kaggle-competitions/kaggle/8089/logos/header.png?t=2018-01-10-17-54-22)","metadata":{}},{"cell_type":"markdown","source":"In this kernel we will take a deep dive into the metric for the Data Science Bowl 2019: Quadratic Weighted Kappa (QWK). This is a popular metric in Kaggle competitions and is especially useful for classification tasks where the classes are hierarchical. For these kind of classification tasks a simple accuracy score does not make much sense. \n\n\nP.S. Feel free to check out [\"Episode 1\" of Understanding The Metric on Root Mean Squared Logarithmic Error (RSMLE)](https://www.kaggle.com/carlolepelaars/understanding-the-metric-rmsle)\n","metadata":{}},{"cell_type":"markdown","source":"## Table of Contents","metadata":{}},{"cell_type":"markdown","source":"- [Dependencies](#1)\n- [Preparation](#2)\n- [The Metric](#3)\n- [Best Baselines](#4)\n- [Optimizing QWK](#5)\n- [Submission](#6)","metadata":{}},{"cell_type":"markdown","source":"## Dependencies <a id=\"1\"></a>","metadata":{}},{"cell_type":"code","source":"# Standard Dependencies\nimport os\nimport scipy as sp\nimport numpy as np\nimport random as rn\nimport pandas as pd\nfrom numba import jit\nfrom functools import partial\n\n# The metric in question\nfrom sklearn.metrics import cohen_kappa_score\n\n# Machine learning\nimport tensorflow as tf\nimport keras.backend as K\nfrom keras.callbacks import Callback\n\n# Set seed for reproducability\nseed = 1234\nrn.seed(seed)\nnp.random.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\n\n# Specify paths\nPATH = \"../input/data-science-bowl-2019/\"\nTRAIN_PATH = PATH + \"train_labels.csv\"\nSUB_PATH = PATH + \"sample_submission.csv\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# File sizes and specifications\nprint('\\n# Files and file sizes')\nfor file in os.listdir(PATH):\n    print('{}| {} MB'.format(file.ljust(30), \n                             str(round(os.path.getsize(PATH + file) / 1000000, 2))))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preparation <a id=\"2\"></a>","metadata":{}},{"cell_type":"code","source":"# Load in data\ndf = pd.read_csv(TRAIN_PATH)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(3)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The Metric <a id=\"3\"></a>","metadata":{}},{"cell_type":"markdown","source":"Kaggle's explanation of Quadratic Weighted Kappa on the [Data Science Bowl 2019 Evaluation page](https://www.kaggle.com/c/data-science-bowl-2019/overview/evaluation):\n\nThe quadratic weighted kappa is calculated as follows. First, an N x N histogram matrix $O$ is constructed, such that $O_{i,j}$ corresponds to the number of installation_ids $i$ (actual) that received a predicted value $j$. An N-by-N matrix of weights, $w$, is calculated based on the difference between actual and predicted values:\n\n$w_{i,j} = \\frac{\\left(i-j\\right)^2}{\\left(N-1\\right)^2}$\n\nAn N-by-N histogram matrix of expected outcomes, $E$, is calculated assuming that there is no correlation between values.  This is calculated as the outer product between the actual histogram vector of outcomes and the predicted histogram vector, normalized such that $E$ and $O$ have the same sum.\n\nFrom these three matrices, the quadratic weighted kappa is calculated as: \n\n$\\kappa=1-\\frac{\\sum_{i,j}w_{i,j}O_{i,j}}{\\sum_{i,j}w_{i,j}E_{i,j}}.$\n\n---------------------------------------------------\n\nNote that Quadratic Weighted Kappa score is a ratio that can take a value between -1 and 1. A negative QWK score implies that the model is \"worse than random\". A random model should give a score of close to 0. Lastly, perfect predictions will yield a score of 1.\n","metadata":{}},{"cell_type":"markdown","source":"Instead of implementing Quadratic Weighted Kappa from scratch we can also get the metric (almost) out-of-the-box from scikit-learn. The only thing we need to specify is that the weights are quadratic.","metadata":{}},{"cell_type":"code","source":"def sklearn_qwk(y_true, y_pred) -> np.float64:\n    \"\"\"\n    Function for measuring Quadratic Weighted Kappa with scikit-learn\n    \n    :param y_true: The ground truth labels\n    :param y_pred: The predicted labels\n    \n    :return The Quadratic Weighted Kappa Score (QWK)\n    \"\"\"\n    return cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"However, Scikit-learn's implementation can be relatively slow. Luckily, [Kaggle Grandmaster CPMP](https://www.kaggle.com/cpmpml) implemented a really fast method to calculate Quadratic Weighted Kappa using the open-source compiler [Numba](http://numba.pydata.org/).\n\n[Source](https://www.kaggle.com/cpmpml/ultra-fast-qwk-calc-method)\n\n[Discussion Topic](https://www.kaggle.com/c/data-science-bowl-2019/discussion/114133#latest-657027)","metadata":{}},{"cell_type":"code","source":"@jit\ndef cpmp_qwk(a1, a2, max_rat=3) -> float:\n    \"\"\"\n    A ultra fast implementation of Quadratic Weighted Kappa (QWK)\n    Source: https://www.kaggle.com/c/data-science-bowl-2019/discussion/114133\n    \n    :param a1: The ground truth labels\n    :param a2: The predicted labels\n    :param max_rat: The maximum target value\n    \n    return: A floating point number with the QWK score\n    \"\"\"\n    assert(len(a1) == len(a2))\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e / a1.shape[0]\n\n    return 1 - o / e","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Best Baselines <a id=\"4\"></a>","metadata":{}},{"cell_type":"markdown","source":"One of the most naive predictions that we can make on this dataset to predict the class that occurs the most. On this dataset that will be 3. The Quadratic Weighted Kappa score will be 0 and therefore no better than random. QWK has a robustness that we also see with a metric such as [The Area under the ROC Curve (AUC)](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5).","metadata":{}},{"cell_type":"code","source":"# Get the ground truth labels\ntrue_labels = df['accuracy_group']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check which labels are present\nprint(\"Label Distribution:\")\ndf['accuracy_group'].value_counts()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate scores for very naive baselines\ndumb_score = sklearn_qwk(true_labels, np.full(len(true_labels), 3))\nrandom_score = round(sklearn_qwk(true_labels, np.random.randint(0, 4, size=len(true_labels))), 5)\nprint(f\"Simply predicting the most common class will yield a QWK score of:\\n{dumb_score}\\n\")\nprint(f\"Random predictions will yield a QWK score of:\\n{random_score}\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we take a closer look at the data we readily notice that there are five different assessments for which we have to predict the accuracy group. To make a good naive prediction we can groupby this assignment and take the mode for each assignment as our prediction. It seems like taking the mean and rounding out will yield good naive predictions. However, this will not yield as good a score as taking the mode.","metadata":{}},{"cell_type":"code","source":"print(\"Assessment types in the training data:\")\nlist(set(df['title']))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group by assessments and take the mode\nmode_mapping = df.groupby('title')['accuracy_group'].agg(lambda x:x.value_counts().index[0])\nmode_preds = df['title'].map(mode_mapping)\n\n# Group by assessments and take the rounded mean\nmean_mapping = df.groupby('title')['accuracy_group'].mean().round()\nmean_preds = df['title'].map(mean_mapping)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check which a score a less naive baseline would give\ngrouped_mode_score = round(sklearn_qwk(true_labels, mode_preds), 5)\ngrouped_mean_score = round(sklearn_qwk(true_labels, mean_preds), 5)\nprint(f\"The naive grouping of the assessments and taking the mode will yield us a QWK score of:\\n\\\n{grouped_mode_score}\")\nprint(f\"The naive grouping of the assessments and taking the rounded mean will yield us a QWK score of:\\n\\\n{grouped_mean_score}\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optimizing QWK <a id=\"5\"></a>","metadata":{}},{"cell_type":"markdown","source":"The most naive way to maximize QWK is to optimize the accuracy. This will give suboptimal results because the accuracy does not take into account small deviations from the target variable. \n\nSo how do optimize QWK in a smart way?\n\nIn general there are two valid ways:\n\n1. Approach the modeling as a regression problem. Minimize the [Mean Squared Error (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error) and round the predictions from the model. Ideally, optimize the rounding thresholds.\n2. Use QWK Directly as a loss function.","metadata":{}},{"cell_type":"markdown","source":"### 1. Approach the modeling as a regression problem.","metadata":{}},{"cell_type":"markdown","source":"While Quadratic Weighted Kappa (QWK) fundamentally is a classification metric. It can be very beneficial to build regression models by minimizing MSE and round the predictions afterwards. This will in general give better results and is a much simples method than trying to implement QWK as a loss function.\n\n","metadata":{}},{"cell_type":"markdown","source":"For example, let's take the mean predictions from the last section but with the rounding. We will use the mode predications to optimize the rounding thresholds and try to improve on the mean predictions.","metadata":{}},{"cell_type":"code","source":"# Map the mean based on the assessment title\nraw_mean_mapping = df.groupby('title')['accuracy_group'].mean()\nraw_mean_preds = df['title'].map(raw_mean_mapping)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now optimize the round thresholds as to maximize the QWK score. When doing this in practice be careful not to use the validation data to optimize the thresholds as this can lead to target leakage.\n\nCredits to [Kaggle Grandmaster Abhishek Thakur](https://www.kaggle.com/abhishek) for creating this \"OptimizedRounder\" class. The original class can be found in [this Kaggle kernel](https://www.kaggle.com/abhishek/optimizer-for-quadratic-weighted-kappa).","metadata":{}},{"cell_type":"code","source":"class OptimizedRounder(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize Quadratic Weighted Kappa (QWK) score\n    \"\"\"\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n        \n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n\n        ll = cohen_kappa_score(y, X_p, weights='quadratic')\n        return -ll\n\n    def fit(self, X, y):\n        \"\"\"\n        Optimize rounding thresholds\n        \n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n        \n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n        return X_p\n\n    def coefficients(self):\n        \"\"\"\n        Return the optimized coefficients\n        \"\"\"\n        return self.coef_['x']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Optimize rounding thresholds (No effect since we have naive baselines)\noptR = OptimizedRounder()\noptR.fit(mode_preds, true_labels)\ncoefficients = optR.coefficients()\nopt_preds = optR.predict(raw_mean_preds, coefficients)\nnew_score = sklearn_qwk(true_labels, opt_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Optimized Thresholds:\\n{coefficients}\\n\")\nprint(f\"The Quadratic Weighted Kappa (QWK)\\n\\\nwith optimized rounding thresholds is: {round(new_score, 5)}\\n\")\nprint(f\"This is an improvement of {round(new_score - grouped_mean_score, 5)} \\\nover the unoptimized rounding.\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unfortunately, in this case we will not improve because the predictions themselves are naive. However, in practice the optimized rounding will slightly improve the final QWK score.","metadata":{}},{"cell_type":"markdown","source":"You can also use this Keras custom Callback to save the model with the highest QWK score.","metadata":{}},{"cell_type":"code","source":"class QWK(Callback):\n    \"\"\"\n    A custom Keras callback for saving the best model\n    according to the Quadratic Weighted Kappa (QWK) metric\n    \"\"\"\n    def __init__(self, model_name=\"model.h5\"):\n        self.model_name = model_name\n    \n    def on_train_begin(self, logs={}):\n        \"\"\"\n        Initialize list of QWK scores on validation data\n        \"\"\"\n        self.val_kappas = []\n\n    def on_epoch_end(self, epoch, logs={}):\n        \"\"\"\n        Gets QWK score on the validation data\n        \n        :param epoch: The current epoch number\n        \"\"\"\n        # Get predictions and convert to integers\n        y_pred, labels = get_preds_and_labels(model, val_generator)\n        y_pred = np.rint(y_pred).astype(np.uint8).clip(0, 4)\n        _val_kappa = cpmp_qwk(labels, y_pred)\n        self.val_kappas.append(_val_kappa)\n        print(f\"val_kappa: {round(_val_kappa, 4)}\")\n        if _val_kappa == max(self.val_kappas):\n            print(\"Validation Kappa has improved. Saving model.\")\n            self.model.save(self.model_name)\n        return\n    \ndef get_preds_and_labels(model, generator):\n    \"\"\"\n    Get predictions and labels from the generator\n    \"\"\"\n    preds = []\n    labels = []\n    for _ in range(int(np.ceil(generator.samples / BATCH_SIZE))):\n        x, y = next(generator)\n        preds.append(model.predict(x))\n        labels.append(y)\n    # Flatten list of numpy arrays\n    return np.concatenate(preds).ravel(), np.concatenate(labels).ravel()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Use QWK Directly as a loss function.","metadata":{}},{"cell_type":"markdown","source":"If you like you can also directly optimize the QWK by using it as a loss function. Here is an implementation for Tensorflow/Keras models.\n\n[Source](https://stackoverflow.com/questions/54831044/how-can-i-specify-a-loss-function-to-be-quadratic-weighted-kappa-in-keras)","metadata":{}},{"cell_type":"code","source":"def _cohen_kappa(y_true, y_pred, num_classes, weights=None, metrics_collections=None, updates_collections=None, name=None):\n    kappa, update_op = tf.contrib.metrics.cohen_kappa(y_true, y_pred, num_classes, weights, metrics_collections, updates_collections, name)\n    K.get_session().run(tf.local_variables_initializer())\n    with tf.control_dependencies([update_op]):\n        kappa = tf.identity(kappa)\n    return kappa\n\ndef cohen_kappa_loss(num_classes, weights=None, metrics_collections=None, updates_collections=None, name=None):\n    \"\"\"\n    A loss function that measures the Quadratic Weighted Kappa (QWK) score\n    and can be used in a Tensorflow / Keras model\n    \"\"\"\n    def cohen_kappa(y_true, y_pred):\n        return -_cohen_kappa(y_true, y_pred, num_classes, weights, metrics_collections, updates_collections, name)\n    return cohen_kappa","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission <a id=\"6\"></a>","metadata":{}},{"cell_type":"markdown","source":"The test data has some assessments that are in the training data, while other assessments are totally new. For the overlapping assessments we will predict the mode calculated on the training data. Once we aggregate on each installation id we get valid naive predictions that we submit to Kaggle.","metadata":{}},{"cell_type":"code","source":"# Read in Test Data\ntest_df = pd.read_csv(PATH + \"test.csv\")\n\n# Map the mode to the test data and create the final predictions through aggregation\ntest_df['preds'] = test_df['title'].map(mode_mapping)\nfinal_preds = test_df.groupby('installation_id')['preds'].agg(lambda x:x.value_counts().index[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make submission for Kaggle\nsub_df = pd.read_csv(SUB_PATH)\nsub_df['accuracy_group'] = list(final_preds.fillna(0).astype(np.uint8))\nsub_df.to_csv(\"submission.csv\", index=False);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Final predictions:')\nsub_df.head(2)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you want to learn more about Quadratic Weighted Kappa I suggest watching [this video from the Coursera course \"How to win Data Science Competitions\"](https://www.coursera.org/lecture/competitive-data-science/classification-metrics-review-EhJzY). The part on QWK starts at 13:00.\n\n\n**That's it! If you like this Kaggle kernel, feel free to give an upvote and leave a comment! Your feedback is also very welcome! I will try to implement your suggestions in this kernel!**","metadata":{}}]}